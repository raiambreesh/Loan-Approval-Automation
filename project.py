# -*- coding: utf-8 -*-
"""MSc Project Hellen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xffa5hwWSSXAnJlpA4LjyKreyUx1GavW

#Imports
"""

# Visualisation Imports
import matplotlib.pyplot as plt        # For plotting and visualisations
import seaborn as sns                  # For statistical visualisations


# Preprocessing Imports
import pandas as pd                   #For data manipulation & analysis
import numpy as np                    #For numerical operations
from sklearn import preprocessing # Import the preprocessing module from scikit-learn for data transformation utilities
from collections import Counter # Import Counter to count occurrences of labels/classes in the dataset
from sklearn.preprocessing import StandardScaler # Import StandardScaler to normalize the feature values (mean = 0, std = 1)
from imblearn.over_sampling import SMOTE   #to balance our data using smote
from sklearn.model_selection import train_test_split # to split the data into test and train sets
from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
)  #For performance metrics
from sklearn.preprocessing import LabelEncoder  # For label encoding

!pip install imbalanced-learn

# Modelling Imports
from sklearn.ensemble import RandomForestClassifier # Import the RandomForestClassifier model from scikit-learn
from sklearn.model_selection import GridSearchCV # Import GridSearchCV for hyperparameter tuning using cross-validation
import shap                            # For SHAP value analysis

!pip install lime

from lime import lime_tabular

from lime.lime_tabular import LimeTabularExplainer

!pip install xgboost

from xgboost import XGBClassifier

"""#Dataset"""

# Load the dataset into a DataFrame
df = pd.read_csv("loans_data.csv")

# Display the first 5 rows of the dataset to understand its structure
df.head()

# Check the shape of the dataset
df.shape

#Check the data types
print(df.dtypes)

# in the Dependents column, Replace '3+' with 3 and convert to numeric
df['Dependents'] = df['Dependents'].replace('3+', 3)
df['Dependents'] = pd.to_numeric(df['Dependents'], errors='coerce')

print(df.dtypes)

"""#Visualisations

"""

# Define the mapping for Credit_History
credit_history_map = {
    0.0: 'Bad Credit History',
    1.0: 'Good Credit History'
}

# Apply the mapping to the 'Credit_History' column
df['Credit_History'] = df['Credit_History'].map(credit_history_map)

# Define the mapping for Loan_Status
loan_status_map = {
    'Y': 'Yes',
    'N': 'No'
}

# Apply the mapping to the 'Loan_Status' column
df['Loan_Status'] = df['Loan_Status'].map(loan_status_map)

# Display the first 5 rows of the dataset
df.head()

"""Loan Status Distribution"""

# Count the values in the 'loan_status' column
loan_status_counts = df['Loan_Status'].value_counts()

# Plot the pie chart
plt.figure(figsize=(6, 6))
plt.pie(loan_status_counts, labels=loan_status_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Loan Status Distribution')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

#autopct='%1.1f%%' shows percentages on the chart.
#startangle=140 rotates the start of the pie chart for aesthetics.

"""Histograms"""

# Histograms for Applicant and Coapplicant Income
plt.figure(figsize=(14, 6))
for i, column in enumerate(['ApplicantIncome', 'CoapplicantIncome']):
    plt.subplot(1, 3, i + 1)
    sns.histplot(df[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.show()

# Histograms for Loan Amount and Loan term
plt.figure(figsize=(14, 6))
for i, column in enumerate(['LoanAmount', 'Loan_Amount_Term']):
    plt.subplot(1, 3, i + 1)
    sns.histplot(df[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.show()

"""Grouped Bar Charts"""

# Loan Status and Gender
sns.countplot(data=df, x='Gender', hue='Loan_Status')
plt.title("Loan Status by Gender")
plt.show()

# Loan Status and Education
sns.countplot(data=df, x='Education', hue='Loan_Status')
plt.title("Loan Status by Education")
plt.show()

# Loan Status and Credit History
sns.countplot(data=df, x='Credit_History', hue='Loan_Status')
plt.title("Loan Status by Credit History")
plt.show()

"""# Correlation Analysis"""

# Create the correlation matrix for all numeric features in the df
correlation_matrix = df.corr(numeric_only=True)

# Create a heatmap using seaborn to visualise the correlation matrix
# 'annot=True' displays the correlation coefficients inside the squares
# 'cmap="coolwarm"' sets the color scheme for better visual distinction of positive and negative correlations
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')

# Add a title to the heatmap
plt.title("Correlation Matrix")

# Display the heatmap plot
plt.show()

"""#Descriptive Statistics"""

# Display summary statistics for numeric columns
df.describe()

# Basic statistics for categorical columns
df.describe(include=['object'])

"""# Data Preprocessing"""

df = pd.read_csv("loans_data.csv")

df.head()

"""Dropping Noisy Attributes"""

# Drop 'Loan_ID' as it is a unique identifier and not useful for prediction
df = df.drop('Loan_ID', axis=1)

# in the Dependents column, Replace '3+' with 3 and convert to numeric
df['Dependents'] = df['Dependents'].replace('3+', 3)
df['Dependents'] = pd.to_numeric(df['Dependents'], errors='coerce')

# Display the first 5 rows of the dataset
df.head()

"""Handling Missing Values"""

# Check for missing values in the dataset
df.isnull().sum()

# Using mode imputation for categorical features
df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])
df['Married'] = df['Married'].fillna(df['Married'].mode()[0])
df['Dependents'] = df['Dependents'].fillna(df['Dependents'].mode()[0])
df['Self_Employed'] = df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])

# Using median imputation for numeric features
df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].median())

df.isnull().sum()

"""Label Encoding"""

# Encode categorical variables using Label Encoding

# Initiate the encoder
le = LabelEncoder()

# Apply label encoding to the 'Gender' column
df['Gender'] = le.fit_transform(df['Gender'])

# Apply label encoding to the 'Married' column
df['Married'] = le.fit_transform(df['Married'])

# Apply label encoding to the 'Education' column
df['Education'] = le.fit_transform(df['Education'])

# Apply label encoding to the 'Self_Employed' column
df['Self_Employed'] = le.fit_transform(df['Self_Employed'])

# Apply label encoding to the 'Property_Area' column
df['Property_Area'] = le.fit_transform(df['Property_Area'])

# Apply label encoding to the target variable 'Loan_Status'
df['Loan_Status'] = le.fit_transform(df['Loan_Status'])

# Display the first 5 rows of the dataset
df.head()

"""Balancing the classes and splitting the train and test sets"""

loan_status_count = df['Loan_Status'].value_counts()
print(loan_status_count)

X = df.iloc[:, :-1].values    # predictor attributes
y = df.iloc[:,-1].values     # target attribute

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
oversample = SMOTE()
X_train_sm, y_train_sm = oversample.fit_resample(X_train, y_train) # using smote

# Check the value counts
count = Counter(y_train_sm)
count

"""#Classification on the Loan_Status feature

# **1. Random Forest Model**

Feature Importance
"""

# Fitting the RF model to the df before balancing
model = RandomForestClassifier()
model.fit(X_train, y_train)

importances = model.feature_importances_

# Use original column names
feature_names = df.drop('Loan_Status', axis=1).columns

# Create and sort DataFrame
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()

"""# **Random Forest with all features**


"""

# Fitting the RF Model to the balanced data
model = RandomForestClassifier()
model.fit(X_train_sm, y_train_sm)

from sklearn.model_selection import cross_val_predict

# Generate cross-validated predictions
y_pred_cv = cross_val_predict(model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC
y_proba_cv = cross_val_predict(model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# === Map class labels 0 -> 'No', 1 -> 'Yes' ===
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named = pd.Series(y_pred_cv).map(label_map)

# Confusion matrix
cm = confusion_matrix(y_true_named, y_pred_named)
print("Confusion Matrix:\n", cm)

# Accuracy
acc = accuracy_score(y_true_named, y_pred_named)
print("Accuracy:", acc)

# Precision
precision = precision_score(y_true_named, y_pred_named, pos_label='Yes')
print("Precision:", precision)

# Recall
recall = recall_score(y_true_named, y_pred_named, pos_label='Yes')
print("Recall:", recall)

# F1 Score
f1 = f1_score(y_true_named, y_pred_named, pos_label='Yes')
print("F1 Score:", f1)

# AUC Score
auc = roc_auc_score(y_train_sm, y_proba_cv)
print("AUC Score:", auc)

# Full Classification Report
print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named))

"""# **Random Forest after feature selection**"""

# Before running this cell, drop noisy attributes, handle missing values, encode categorical features
# Drop low-importance features and retrain the model to compare performance
low_importance_features = ['Self_Employed', 'Gender', 'Education']
df_reduced = df.drop(columns=low_importance_features)

df_reduced.head()

loan_status_count = df_reduced['Loan_Status'].value_counts()
print(loan_status_count)

X = df_reduced.iloc[:, :-1].values    # predictor attributes
y = df_reduced.iloc[:,-1].values     # target attribute

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
oversample = SMOTE()
X_train_sm, y_train_sm = oversample.fit_resample(X_train, y_train) # using smote

# Check the value counts
count = Counter(y_train_sm)
count

# Fitting the RF Model to the balanced and scaled data after feature selection
model = RandomForestClassifier()
model.fit(X_train_sm, y_train_sm)

# Generate cross-validated predictions
y_pred_cv = cross_val_predict(model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC
y_proba_cv = cross_val_predict(model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# === Map class labels 0 -> 'No', 1 -> 'Yes' ===
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named = pd.Series(y_pred_cv).map(label_map)

# Confusion matrix
cm = confusion_matrix(y_true_named, y_pred_named)
print("Confusion Matrix:\n", cm)

# Accuracy
acc = accuracy_score(y_true_named, y_pred_named)
print("Accuracy:", acc)

# Precision
precision = precision_score(y_true_named, y_pred_named, pos_label='Yes')
print("Precision:", precision)

# Recall
recall = recall_score(y_true_named, y_pred_named, pos_label='Yes')
print("Recall:", recall)

# F1 Score
f1 = f1_score(y_true_named, y_pred_named, pos_label='Yes')
print("F1 Score:", f1)

# AUC Score
auc = roc_auc_score(y_train_sm, y_proba_cv)
print("AUC Score:", auc)

# Full Classification Report
print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named))

"""# **Hyperparameter Tuning for RF Model**"""

# Defining the parameter grid
param_grid = {
    'max_depth': [5, 10, 15, 20, None],  # Varying depth levels for different levels of complexity
    'n_estimators': [50, 100, 150],     # Number of trees in the forest
    'min_samples_split': [2, 10, 20],    # Minimum samples required to split a node
    'min_samples_leaf': [1, 2, 4],       # Minimum samples required at a leaf node
    'max_features': ['sqrt', 'log2'],    # Max number of features considered for splitting
    'bootstrap': [True, False]           # Whether bootstrap samples are used when building trees
}

# Creating the model
rf = RandomForestClassifier(random_state=42)

# Set up grid search for hyperparameter tuning
grid_search = GridSearchCV(
    estimator=rf,            # The base model: RandomForestClassifier
    param_grid=param_grid,   # Dictionary of hyperparameter(s) to test
    cv=5,                    # 5-fold cross-validation to evaluate each setting
    scoring='f1',            # Use f1 score to evaluate model performance
    n_jobs=-1,               # Use all available CPU cores for faster computation
    verbose=1                # Show progress of the grid search in the console
)

# Fitting the grid search to the data
grid_search.fit(X_train_sm, y_train_sm)

# Getting the best parameters
print("Best Parameters:", grid_search.best_params_)
print("Best F1 Score:", grid_search.best_score_)

# Training the final model with best parameters

final_model = RandomForestClassifier(
    bootstrap=False,          # No bootstrapping
    max_depth=10,             # Max depth of the trees
    max_features='sqrt',      # Max features considered for splitting
    min_samples_leaf=4,       # Minimum samples at leaf nodes
    min_samples_split=2,      # Minimum samples to split an internal node
    n_estimators=150,         # Number of trees in the forest
    random_state=42           # Ensuring reproducibility
)

# Fit the model to the balanced and scaled training data
final_model.fit(X_train_sm, y_train_sm)

# Generate cross-validated predictions
y_pred_cv = cross_val_predict(final_model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC
y_proba_cv = cross_val_predict(final_model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# === Map class labels 0 -> 'No', 1 -> 'Yes' ===
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named = pd.Series(y_pred_cv).map(label_map)

# Confusion matrix
cm = confusion_matrix(y_true_named, y_pred_named)
print("Confusion Matrix:\n", cm)

# Accuracy
acc = accuracy_score(y_true_named, y_pred_named)
print("Accuracy:", acc)

# Precision
precision = precision_score(y_true_named, y_pred_named, pos_label='Yes')
print("Precision:", precision)

# Recall
recall = recall_score(y_true_named, y_pred_named, pos_label='Yes')
print("Recall:", recall)

# F1 Score
f1 = f1_score(y_true_named, y_pred_named, pos_label='Yes')
print("F1 Score:", f1)

# AUC Score
auc = roc_auc_score(y_train_sm, y_proba_cv)
print("AUC Score:", auc)

# Full Classification Report
print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named))

"""# **2. XGBoost Model**"""

#Creating and fitting the model
xgb_model = XGBClassifier(
    eval_metric='logloss',   # Recommended for binary classification
    random_state=42
)

xgb_model.fit(X_train_sm, y_train_sm)

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

from sklearn.model_selection import cross_val_predict

# Generate cross-validated predictions (label predictions)
y_pred_cv_xgb = cross_val_predict(xgb_model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC (probability of class 1 — 'Yes')
y_proba_cv_xgb = cross_val_predict(xgb_model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# Map class labels 0 -> 'No', 1 -> 'Yes'
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named_xgb = pd.Series(y_pred_cv_xgb).map(label_map)

# Confusion matrix
cm_xgb = confusion_matrix(y_true_named, y_pred_named_xgb)
print("Confusion Matrix:\n", cm_xgb)

# Accuracy
acc_xgb = accuracy_score(y_true_named, y_pred_named_xgb)
print("Accuracy:", acc_xgb)

# Precision
precision_xgb = precision_score(y_true_named, y_pred_named_xgb, pos_label='Yes')
print("Precision:", precision_xgb)

# Recall
recall_xgb = recall_score(y_true_named, y_pred_named_xgb, pos_label='Yes')
print("Recall:", recall_xgb)

# F1 Score
f1_xgb = f1_score(y_true_named, y_pred_named_xgb, pos_label='Yes')
print("F1 Score:", f1_xgb)

# AUC Score
auc_xgb = roc_auc_score(y_train_sm, y_proba_cv_xgb)
print("AUC Score:", auc_xgb)

# Classification Report
print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named_xgb))

"""# **XGBOOST Hyperparameter Tuning**

"""

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

xgb = XGBClassifier(random_state=42, eval_metric='logloss')

grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_sm, y_train_sm)

print("Best Parameters:", grid_search.best_params_)
print("Best F1 Score:", grid_search.best_score_)

best_xgb_model = grid_search.best_estimator_

# Generate cross-validated predictions
y_pred_cv = cross_val_predict(best_xgb_model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC
y_proba_cv = cross_val_predict(best_xgb_model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# Map class labels
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named = pd.Series(y_pred_cv).map(label_map)

# Confusion matrix
cm = confusion_matrix(y_true_named, y_pred_named)
print("Confusion Matrix:\n", cm)

# Accuracy
print("Accuracy:", accuracy_score(y_true_named, y_pred_named))

# Precision
print("Precision:", precision_score(y_true_named, y_pred_named, pos_label='Yes'))

# Recall
print("Recall:", recall_score(y_true_named, y_pred_named, pos_label='Yes'))

# F1 Score
print("F1 Score:", f1_score(y_true_named, y_pred_named, pos_label='Yes'))

# AUC Score
print("AUC Score:", roc_auc_score(y_train_sm, y_proba_cv))

# Full report
print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named))

"""# **3. Ensemble Model**

# **Stacking Ensemble Model**
"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Base learners
estimators = [
    ('rf', final_model),  # already tuned
    ('xgb', best_xgb_model)  # already tuned
]

# Meta-learner (final estimator)
meta_learner = LogisticRegression()

# Stacking classifier
stacking_model = StackingClassifier(
    estimators=estimators,
    final_estimator=meta_learner,
    cv=5,
    n_jobs=-1,
    passthrough=False
)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report
)

# Cross-validated predictions
y_pred_cv_stack = cross_val_predict(stacking_model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC
y_proba_cv_stack = cross_val_predict(stacking_model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# Map labels
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named_stack = pd.Series(y_pred_cv_stack).map(label_map)

# Metrics
cm_stack = confusion_matrix(y_true_named, y_pred_named_stack)
print("Confusion Matrix:\n", cm_stack)

acc_stack = accuracy_score(y_true_named, y_pred_named_stack)
print("Accuracy:", acc_stack)

precision_stack = precision_score(y_true_named, y_pred_named_stack, pos_label='Yes')
print("Precision:", precision_stack)

recall_stack = recall_score(y_true_named, y_pred_named_stack, pos_label='Yes')
print("Recall:", recall_stack)

f1_stack = f1_score(y_true_named, y_pred_named_stack, pos_label='Yes')
print("F1 Score:", f1_stack)

auc_stack = roc_auc_score(y_train_sm, y_proba_cv_stack)
print("AUC Score:", auc_stack)

print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named_stack))

"""#**Voting Ensemble** **Model**"""

from sklearn.ensemble import VotingClassifier

# Build the voting ensemble
ensemble_model = VotingClassifier(
    estimators=[('rf', final_model), ('xgb', xgb_model)],
    voting='soft'
)

# Fit the ensemble model
ensemble_model.fit(X_train_sm, y_train_sm)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import (
    confusion_matrix, accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report
)

# Generate cross-validated predictions (label predictions)
y_pred_cv_ens = cross_val_predict(ensemble_model, X_train_sm, y_train_sm, cv=5, method='predict')

# Probabilities for AUC (probability of class 1 — 'Yes')
y_proba_cv_ens = cross_val_predict(ensemble_model, X_train_sm, y_train_sm, cv=5, method='predict_proba')[:, 1]

# Map class labels 0 -> 'No', 1 -> 'Yes'
label_map = {0: 'No', 1: 'Yes'}
y_true_named = pd.Series(y_train_sm).map(label_map)
y_pred_named_ens = pd.Series(y_pred_cv_ens).map(label_map)

# Confusion matrix
cm_ens = confusion_matrix(y_true_named, y_pred_named_ens)
print("Confusion Matrix:\n", cm_ens)

# Accuracy
acc_ens = accuracy_score(y_true_named, y_pred_named_ens)
print("Accuracy:", acc_ens)

# Precision
precision_ens = precision_score(y_true_named, y_pred_named_ens, pos_label='Yes')
print("Precision:", precision_ens)

# Recall
recall_ens = recall_score(y_true_named, y_pred_named_ens, pos_label='Yes')
print("Recall:", recall_ens)

# F1 Score
f1_ens = f1_score(y_true_named, y_pred_named_ens, pos_label='Yes')
print("F1 Score:", f1_ens)

# AUC Score
auc_ens = roc_auc_score(y_train_sm, y_proba_cv_ens)
print("AUC Score:", auc_ens)

# Classification Report
print("\nClassification Report:\n", classification_report(y_true_named, y_pred_named_ens))

"""# **LIME explanation for Random Forest Tuned Model**"""

# Get feature names if you dropped 'Loan_Status'
feature_names = df.drop('Loan_Status', axis=1).columns.tolist()

explainer = LimeTabularExplainer(
    training_data=X_train_sm,
    feature_names=feature_names,
    class_names=['No', 'Yes'],
    mode='classification',
    discretize_continuous=True
)

#Choose an instance to explain
i = 4  # for example
instance = X_train_sm[i]

explanation = explainer.explain_instance(
    data_row=instance,
    predict_fn=final_model.predict_proba,
    num_features=10
)
explanation.show_in_notebook()

explanation.as_pyplot_figure()

#Choose an instance to explain
i = 6  # for example
instance = X_train_sm[i]

explanation = explainer.explain_instance(
    data_row=instance,
    predict_fn=final_model.predict_proba,
    num_features=10
)
explanation.show_in_notebook()

explanation.as_pyplot_figure()